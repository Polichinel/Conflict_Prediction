{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use adaboost, Gradient Tree Boosting and xgboosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES:\n",
    "Der er nogle ting du skal tilbage og havde styr på. \n",
    "\n",
    "Baglens:\n",
    "\n",
    "- Du skal have flere variabler med herover den først runde feature selection skal kun være helt grov ect vægle mellem log eller ike log ect.\n",
    "- Der er pludseligt en helt masse missing i dit y!? wth! får lige styr på den kat..\n",
    "- i forgående gem df som goepandas.dataframe i pickle\n",
    "- i forgående gem df (- geometry) som padas.dataframe i csv\n",
    "- ændre navn fatilities i forgående og lav også en logged version.\n",
    "- Se generelt din estimation for hvad der giver mening som logged...\n",
    "\n",
    "forlens:\n",
    "\n",
    "- 1000 random less-unbalanced xgboosts to get a distribution of metric results\n",
    "- tree graph\n",
    "- world map of predict vs true. (proba vs log(best) t+1 or pred vs binary_best t +1)\n",
    "- world map of diff between binary pred and true.\n",
    "- Use the true hold-out-set.\n",
    "- Robustness test with only new onsets\n",
    "- you can easely do basian correction: even country (or something) specific\n",
    "\n",
    "$$P(død)=\\frac{xgboost\\_est\\_(dist) * country\\_mean }{xgboost\\_est\\_(dist) * country\\_mean + (1-xgboost\\_est\\_(dist)) * (1-country\\_mean) }$$\n",
    "\n",
    "men hvis du vil gøre det med dists kan du vel ikke bruge $1-$..\n",
    "\n",
    "- det må være et empirisk spørgsmål om 1/1, 1/30, 1/1 med B-correction eller 1/30 med B-correction er bedst.\n",
    "\n",
    "- tjek if your model gets worse the more years we move away from 2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import xgboost as xgb \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc, average_precision_score, recall_score, precision_recall_curve, roc_auc_score, roc_curve, precision_score, accuracy_score\n",
    "from sklearn.utils.fixes import signature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pkl_file = open('full_df.pkl', 'rb')\n",
    "\n",
    "df = pickle.load(pkl_file)\n",
    "\n",
    "pkl_file.close()\n",
    "\n",
    "df.dropna(inplace = True) # do to some unknown fuckery in the last script.. to fix later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1267740, 62)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_labels = ['past_fatalities', 'fatilities_country_year', 'nearest_conflict','past_conflicts',\n",
    "            'landarea','country_area_sum',\n",
    "            'interp_pop_gpw_sum','country_pop_sum',         \n",
    "            'light_capita_cell','light_capita_country',\n",
    "            'low_ratio_light', 'diff_median_light',\n",
    "            'mountains_mean', 'ttime_mean', 'capdist',\n",
    "            'bdist1',\n",
    "            'excluded_binary',\n",
    "            'petroleum_full']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Du mangler stadigt at gemme din \"posterior\" aka alle forudsiagte y_tilde (i out_years modellerne.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict of metrics\n",
    "\n",
    "def cons_dict_of_metric():\n",
    "    # Construct the empty dict of matrics and other stuff\n",
    "    dict_of_metrics = {}\n",
    "    dict_of_metrics['auc'] = []\n",
    "    dict_of_metrics['ap'] = []\n",
    "\n",
    "    dict_of_metrics['recall50'] = []\n",
    "    dict_of_metrics['recall75'] = []\n",
    "    dict_of_metrics['recall95'] = []\n",
    "\n",
    "    dict_of_metrics['precision50'] = []\n",
    "    dict_of_metrics['precision75'] = []\n",
    "    dict_of_metrics['precision95'] = []\n",
    "    \n",
    "    dict_of_metrics['accuracy50'] = []\n",
    "    \n",
    "    #roc,rp, feature imp\n",
    "    \n",
    "    dict_of_metrics['roc'] = []\n",
    "    dict_of_metrics['prc'] = []\n",
    "    dict_of_metrics['feature_imp'] = []\n",
    "    \n",
    "    return(dict_of_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_years_estimation(sample_size = 100, balance_parameter = 1):    \n",
    "    \n",
    "    # Hold out years:\n",
    "    df_test_years = df[df['year']>2005].copy()\n",
    "    df_train_years = df[df['year']<=2005].copy()\n",
    "\n",
    "    #radnom seeds for loop (culd do without, but it eases intpr.)\n",
    "    random_seeds = np.random.randint(1000,9999,1000)\n",
    "\n",
    "    # dict of metrics\n",
    "    dict_of_metrics = cons_dict_of_metric()    \n",
    "    dict_of_metrics['preds1'] = []\n",
    "    dict_of_metrics['preds0'] = []\n",
    "\n",
    "    y_label = 'lead_binary_best'\n",
    "\n",
    "    df_train_years_1 = df_train_years[df_train_years['lead_binary_best'] == 1] # events\n",
    "\n",
    "\n",
    "    #for loop\n",
    "    for i in range(sample_size):\n",
    "\n",
    "        # The data:\n",
    "        # the non_events! rigth now 1000 percent\n",
    "        df_train_years_0 = df_train_years[df_train_years['lead_binary_best'] == 0].sample(df_train_years_1.shape[0]*balance_parameter, random_state = random_seeds[i]) \n",
    "        # you shoul dmeak one with *1 and *20 manke it an input in the function \n",
    "\n",
    "        #Merge events and non-events; rigth now 10/100\n",
    "        df_train_years_01 = pd.concat([df_train_years_0,df_train_years_1])\n",
    "\n",
    "        # dfine X and y\n",
    "        X_train = df_train_years_01[X_labels]\n",
    "\n",
    "        y_train = df_train_years_01[y_label]\n",
    "\n",
    "        # test_train: TEMP:\n",
    "        # Create the training and test sets\n",
    "        #X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=random_seeds[i])\n",
    "\n",
    "        X_test = df_test_years[X_labels]\n",
    "        y_test = df_test_years[y_label]\n",
    "\n",
    "        # the model\n",
    "        # Instantiate the XGBClassifier: xg_cl\n",
    "        xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, seed=random_seeds[i])\n",
    "\n",
    "        # Fit the classifier to the training set\n",
    "        xg_cl.fit(X_train, y_train)\n",
    "\n",
    "        # Predict the labels of the test set: preds\n",
    "        preds = xg_cl.predict_proba(X_test)\n",
    "\n",
    "        preds50 = (preds[:,1] >= 0.5)*1 # binary class with thresshold at 0.5 (standart)\n",
    "        preds75 = (preds[:,1] >= 0.75)*1 # binary class with thresshold at 0.75 (standart)\n",
    "        preds95 = (preds[:,1] >= 0.95)*1 # binary class with thresshold at 0.95 (standart)\n",
    "\n",
    "        dict_of_metrics['ap'].append(average_precision_score(y_test,preds[:,1]))\n",
    "        dict_of_metrics['auc'].append(roc_auc_score(y_test,preds[:,1]))\n",
    "\n",
    "        dict_of_metrics['recall50'].append(recall_score(y_test, preds50))\n",
    "        dict_of_metrics['recall75'].append(recall_score(y_test, preds75))\n",
    "        dict_of_metrics['recall95'].append(recall_score(y_test, preds95))\n",
    "\n",
    "        dict_of_metrics['precision50'].append(precision_score(y_test, preds50))\n",
    "        dict_of_metrics['precision75'].append(precision_score(y_test, preds75))\n",
    "        dict_of_metrics['precision95'].append(precision_score(y_test, preds95))\n",
    "        \n",
    "        dict_of_metrics['accuracy50'].append(accuracy_score(y_test,preds50))\n",
    "        \n",
    "        dict_of_metrics['prc'].append(precision_recall_curve(y_test,preds[:,1]))\n",
    "        dict_of_metrics['roc'].append(roc_curve(y_test, preds[:, 1]))\n",
    "    \n",
    "        dict_of_metrics['feature_imp'].append(xg_cl.feature_importances_)\n",
    "        \n",
    "        dict_of_metrics['preds1'].append(preds[:,1]) # nice to have both columns for the baysian correction\n",
    "        dict_of_metrics['preds0'].append(preds[:,0]) # nice to have both columns for the baysian correction\n",
    "\n",
    "        print(i, end='_')\n",
    "\n",
    "    return(dict_of_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_years_estimation_onsets(sample_size = 100, balance_parameter = 1):   \n",
    "    \n",
    "    df_ongoing = df[(df['lead_binary_best']==1)&(df['binary_best'] == 1)]\n",
    "    df_onset = df[~((df['lead_binary_best']==1)&(df['binary_best'] == 1))]\n",
    "    \n",
    "    print('Number of observation/conflicts discarded: {}'.format(df_ongoing.shape[0]))\n",
    "    print('Number of observation preserved: {}'.format(df_onset.shape[0]))\n",
    "    print('Number of conflicts preserved: {}'.format(df_onset[df_onset['lead_binary_best'] == 1].shape[0]))\n",
    "    \n",
    "    # Hold out years:\n",
    "    df_test_years = df_onset[df_onset['year']>2005].copy()\n",
    "    df_train_years = df_onset[df_onset['year']<=2005].copy()\n",
    "\n",
    "    #radnom seeds for loop (culd do without, but it eases intpr.)\n",
    "    random_seeds = np.random.randint(1000,9999,1000)\n",
    "\n",
    "    # dict of metrics\n",
    "    dict_of_metrics = cons_dict_of_metric()    \n",
    "    dict_of_metrics['preds1'] = []\n",
    "    dict_of_metrics['preds0'] = []\n",
    "\n",
    "\n",
    "    y_label = 'lead_binary_best'\n",
    "    \n",
    "    \n",
    "    df_train_years_1 = df_train_years[df_train_years['lead_binary_best'] == 1] # events\n",
    "\n",
    "\n",
    "    #for loop\n",
    "    for i in range(sample_size):\n",
    "\n",
    "        # The data:\n",
    "        # the non_events! rigth now 1000 percent\n",
    "        df_train_years_0 = df_train_years[df_train_years['lead_binary_best'] == 0].sample(df_train_years_1.shape[0]*balance_parameter, random_state = random_seeds[i]) \n",
    "        # you shoul dmeak one with *1 and *20 manke it an input in the function \n",
    "\n",
    "        #Merge events and non-events; rigth now 10/100\n",
    "        df_train_years_01 = pd.concat([df_train_years_0,df_train_years_1])\n",
    "\n",
    "        # dfine X and y\n",
    "        X_train = df_train_years_01[X_labels]\n",
    "\n",
    "        y_train = df_train_years_01[y_label]\n",
    "\n",
    "        # test_train: TEMP:\n",
    "        # Create the training and test sets\n",
    "        #X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=random_seeds[i])\n",
    "\n",
    "        X_test = df_test_years[X_labels]\n",
    "        y_test = df_test_years[y_label]\n",
    "\n",
    "        # the model\n",
    "        # Instantiate the XGBClassifier: xg_cl\n",
    "        xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, seed=random_seeds[i])\n",
    "\n",
    "        # Fit the classifier to the training set\n",
    "        xg_cl.fit(X_train, y_train)\n",
    "\n",
    "        # Predict the labels of the test set: preds\n",
    "        preds = xg_cl.predict_proba(X_test)\n",
    "\n",
    "        preds50 = (preds[:,1] >= 0.5)*1 # binary class with thresshold at 0.5 (standart)\n",
    "        preds75 = (preds[:,1] >= 0.75)*1 # binary class with thresshold at 0.75 (standart)\n",
    "        preds95 = (preds[:,1] >= 0.95)*1 # binary class with thresshold at 0.95 (standart)\n",
    "\n",
    "        dict_of_metrics['ap'].append(average_precision_score(y_test,preds[:,1]))\n",
    "        dict_of_metrics['auc'].append(roc_auc_score(y_test,preds[:,1]))\n",
    "\n",
    "        dict_of_metrics['recall50'].append(recall_score(y_test, preds50))\n",
    "        dict_of_metrics['recall75'].append(recall_score(y_test, preds75))\n",
    "        dict_of_metrics['recall95'].append(recall_score(y_test, preds95))\n",
    "\n",
    "        dict_of_metrics['precision50'].append(precision_score(y_test, preds50))\n",
    "        dict_of_metrics['precision75'].append(precision_score(y_test, preds75))\n",
    "        dict_of_metrics['precision95'].append(precision_score(y_test, preds95))\n",
    "\n",
    "        dict_of_metrics['accuracy50'].append(accuracy_score(y_test,preds50))\n",
    "        \n",
    "        dict_of_metrics['prc'].append(precision_recall_curve(y_test,preds[:,1]))\n",
    "        dict_of_metrics['roc'].append(roc_curve(y_test, preds[:, 1]))\n",
    "    \n",
    "        dict_of_metrics['feature_imp'].append(xg_cl.feature_importances_)\n",
    "        \n",
    "        #dict_of_metrics['preds'].append(preds) # nice to have both columns for the baysian correction\n",
    "        dict_of_metrics['preds1'].append(preds[:,1]) # nice to have both columns for the baysian correction\n",
    "        dict_of_metrics['preds0'].append(preds[:,0]) # nice to have both columns for the baysian correction\n",
    "        \n",
    "        print(i, end='_')\n",
    "\n",
    "    return(dict_of_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimates:\n",
    "\n",
    "But you still need to go back and optimize the hyper parameters. But that is not paramount for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43_44_45_46_47_48_49_50_51_52_53_54_55_56_57_58_59_60_61_62_63_64_65_66_67_68_69_70_71_72_73_74_75_76_77_78_79_80_81_82_83_84_85_86_87_88_89_90_91_92_93_94_95_96_97_98_99_100_101_102_103_104_105_106_107_108_109_110_111_112_113_114_115_116_117_118_119_120_121_122_123_124_125_126_127_128_129_130_131_132_133_134_135_136_137_138_139_140_141_142_143_144_145_146_147_148_149_150_151_152_153_154_155_156_157_158_159_160_161_162_163_164_165_166_167_168_169_170_171_172_173_174_175_176_177_178_179_180_181_182_183_184_185_186_187_188_189_190_191_192_193_194_195_196_197_198_199_200_201_202_203_204_205_206_207_208_209_210_211_212_213_214_215_216_217_218_219_220_221_222_223_224_225_226_227_228_229_230_231_232_233_234_235_236_237_238_239_240_241_242_243_244_245_246_247_248_249_250_251_252_253_254_255_256_257_258_259_260_261_262_263_264_265_266_267_268_269_270_271_272_273_274_275_276_277_278_279_280_281_282_283_284_285_286_287_288_289_290_291_292_293_294_295_296_297_298_299_300_301_302_303_304_305_306_307_308_309_310_311_312_313_314_315_316_317_318_319_320_321_322_323_324_325_326_327_328_329_330_331_332_333_334_335_336_337_338_339_340_341_342_343_344_345_346_347_348_349_350_351_352_353_354_355_356_357_358_359_360_361_362_363_364_365_366_367_368_369_370_371_372_373_374_375_376_377_378_379_380_381_382_383_384_385_386_387_388_389_390_391_392_393_394_395_396_397_398_399_400_401_402_403_404_405_406_407_408_409_410_411_412_413_414_415_416_417_418_419_420_421_422_423_424_425_426_427_428_429_430_431_432_433_434_435_436_437_438_439_440_441_442_443_444_445_446_447_448_449_450_451_452_453_454_455_456_457_458_459_460_461_462_463_464_465_466_467_468_469_470_471_472_473_474_475_476_477_478_479_480_481_482_483_484_485_486_487_488_489_490_491_492_493_494_495_496_497_498_499_500_501_502_503_504_505_506_507_508_509_510_511_512_513_514_515_516_517_518_519_520_521_522_523_524_525_526_527_528_529_530_531_532_533_534_535_536_537_538_539_540_541_542_543_544_545_546_547_548_549_550_551_552_553_554_555_556_557_558_559_560_561_562_563_564_565_566_567_568_569_570_571_572_573_574_575_576_577_578_579_580_581_582_583_584_585_586_587_588_589_590_591_592_593_594_595_596_597_598_599_600_601_602_603_604_605_606_607_608_609_610_611_612_613_614_615_616_617_618_619_620_621_622_623_624_625_626_627_628_629_630_631_632_633_634_635_636_637_638_639_640_641_642_643_644_645_646_647_648_649_650_651_652_653_654_655_656_657_658_659_660_661_662_663_664_665_666_667_668_669_670_671_672_673_674_675_676_677_678_679_680_681_682_683_684_685_686_687_688_689_690_691_692_693_694_695_696_697_698_699_700_701_702_703_704_705_706_707_708_709_710_711_712_713_714_715_716_717_718_719_720_721_722_723_724_725_726_727_728_729_730_731_732_733_734_735_736_737_738_739_740_741_742_743_744_745_746_747_748_749_750_751_752_753_754_755_756_757_758_759_760_761_762_763_764_765_766_767_768_769_770_771_772_773_774_775_776_777_778_779_780_781_782_783_784_785_786_787_788_789_790_791_792_793_794_795_796_797_798_799_800_801_802_803_804_805_806_807_808_809_810_811_812_813_814_815_816_817_818_819_820_821_822_823_824_825_826_827_828_829_830_831_832_833_834_835_836_837_838_839_840_841_842_843_844_845_846_847_848_849_850_851_852_853_854_855_856_857_858_859_860_861_862_863_864_865_866_867_868_869_870_871_872_873_874_875_876_877_878_879_880_881_882_883_884_885_886_887_888_889_890_891_892_893_894_895_896_897_898_899_900_901_902_903_904_905_906_907_908_909_910_911_912_913_914_915_916_917_918_919_920_921_922_923_924_925_926_927_928_929_930_931_932_933_934_935_936_937_938_939_940_941_942_943_944_945_946_947_948_949_950_951_952_953_954_955_956_957_958_959_960_961_962_963_964_965_966_967_968_969_970_971_972_973_974_975_976_977_978_979_980_981_982_983_984_985_986_987_988_989_990_991_992_993_994_995_996_997_998_999_"
     ]
    }
   ],
   "source": [
    "dict_of_metrics_out_years = out_years_estimation(1000)\n",
    "\n",
    "file_name = \"dict_of_metrics_out_years.pkl\"\n",
    "output = open(file_name, 'wb') #\n",
    "pickle.dump(dict_of_metrics_out_years, output)\n",
    "output.close()\n",
    "\n",
    "del(dict_of_metrics_out_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observation/conflicts discarded: 6887\n",
      "Number of observation preserved: 1260853\n",
      "Number of conflicts preserved: 6410\n",
      "0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43_44_45_46_47_48_49_50_51_52_53_54_55_56_57_58_59_60_61_62_63_64_65_66_67_68_69_70_71_72_73_74_75_76_77_78_79_80_81_82_83_84_85_86_87_88_89_90_91_92_93_94_95_96_97_98_99_100_101_102_103_104_105_106_107_108_109_110_111_112_113_114_115_116_117_118_119_120_121_122_123_124_125_126_127_128_129_130_131_132_133_134_135_136_137_138_139_140_141_142_143_144_145_146_147_148_149_150_151_152_153_154_155_156_157_158_159_160_161_162_163_164_165_166_167_168_169_170_171_172_173_174_175_176_177_178_179_180_181_182_183_184_185_186_187_188_189_190_191_192_193_194_195_196_197_198_199_200_201_202_203_204_205_206_207_208_209_210_211_212_213_214_215_216_217_218_219_220_221_222_223_224_225_226_227_228_229_230_231_232_233_234_235_236_237_238_239_240_241_242_243_244_245_246_247_248_249_250_251_252_253_254_255_256_257_258_259_260_261_262_263_264_265_266_267_268_269_270_271_272_273_274_275_276_277_278_279_280_281_282_283_284_285_286_287_288_289_290_291_292_293_294_295_296_297_298_299_300_301_302_303_304_305_306_307_308_309_310_311_312_313_314_315_316_317_318_319_320_321_322_323_324_325_326_327_328_329_330_331_332_333_334_335_336_337_338_339_340_341_342_343_344_345_346_347_348_349_350_351_352_353_354_355_356_357_358_359_360_361_362_363_364_365_366_367_368_369_370_371_372_373_374_375_376_377_378_379_380_381_382_383_384_385_386_387_388_389_390_391_392_393_394_395_396_397_398_399_400_401_402_403_404_405_406_407_408_409_410_411_412_413_414_415_416_417_418_419_420_421_422_423_424_425_426_427_428_429_430_431_432_433_434_435_436_437_438_439_440_441_442_443_444_445_446_447_448_449_450_451_452_453_454_455_456_457_458_459_460_461_462_463_464_465_466_467_468_469_470_471_472_473_474_475_476_477_478_479_480_481_482_483_484_485_486_487_488_489_490_491_492_493_494_495_496_497_498_499_500_501_502_503_504_505_506_507_508_509_510_511_512_513_514_515_516_517_518_519_520_521_522_523_524_525_526_527_528_529_530_531_532_533_534_535_536_537_538_539_540_541_542_543_544_545_546_547_548_549_550_551_552_553_554_555_556_557_558_559_560_561_562_563_564_565_566_567_568_569_570_571_572_573_574_575_576_577_578_579_580_581_582_583_584_585_586_587_588_589_590_591_592_593_594_595_596_597_598_599_600_601_602_603_604_605_606_607_608_609_610_611_612_613_614_615_616_617_618_619_620_621_622_623_624_625_626_627_628_629_630_631_632_633_634_635_636_637_638_639_640_641_642_643_644_645_646_647_648_649_650_651_652_653_654_655_656_657_658_659_660_661_662_663_664_665_666_667_668_669_670_671_672_673_674_675_676_677_678_679_680_681_682_683_684_685_686_687_688_689_690_691_692_693_694_695_696_697_698_699_700_701_702_703_704_705_706_707_708_709_710_711_712_713_714_715_716_717_718_719_720_721_722_723_724_725_726_727_728_729_730_731_732_733_734_735_736_737_738_739_740_741_742_743_744_745_746_747_748_749_750_751_752_753_754_755_756_757_758_759_760_761_762_763_764_765_766_767_768_769_770_771_772_773_774_775_776_777_778_779_780_781_782_783_784_785_786_787_788_789_790_791_792_793_794_795_796_797_798_799_800_801_802_803_804_805_806_807_808_809_810_811_812_813_814_815_816_817_818_819_820_821_822_823_824_825_826_827_828_829_830_831_832_833_834_835_836_837_838_839_840_841_842_843_844_845_846_847_848_849_850_851_852_853_854_855_856_857_858_859_860_861_862_863_864_865_866_867_868_869_870_871_872_873_874_875_876_877_878_879_880_881_882_883_884_885_886_887_888_889_890_891_892_893_894_895_896_897_898_899_900_901_902_903_904_905_906_907_908_909_910_911_912_913_914_915_916_917_918_919_920_921_922_923_924_925_926_927_928_929_930_931_932_933_934_935_936_937_938_939_940_941_942_943_944_945_946_947_948_949_950_951_952_953_954_955_956_957_958_959_960_961_962_963_964_965_966_967_968_969_970_971_972_973_974_975_976_977_978_979_980_981_982_983_984_985_986_987_988_989_990_991_992_993_994_995_996_997_998_999_"
     ]
    }
   ],
   "source": [
    "dict_of_metrics_out_years_onsets = out_years_estimation_onsets(1000)\n",
    "\n",
    "file_name = \"dict_of_metrics_out_years_onsets.pkl\"\n",
    "output = open(file_name, 'wb') #\n",
    "pickle.dump(dict_of_metrics_out_years_onsets, output)\n",
    "output.close()\n",
    "\n",
    "del(dict_of_metrics_out_years_onsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43_44_45_46_47_48_49_50_51_52_53_54_55_56_57_58_59_60_61_62_63_64_65_66_67_68_69_70_71_72_73_74_75_76_77_78_79_80_81_82_83_84_85_86_87_88_89_90_91_92_93_94_95_96_97_98_99_100_101_102_103_104_105_106_107_108_109_110_111_112_113_114_115_116_117_118_119_120_121_122_123_124_125_126_127_128_129_130_131_132_133_134_135_136_137_138_139_140_141_142_143_144_145_146_147_148_149_150_151_152_153_154_155_156_157_158_159_160_161_162_163_164_165_166_167_168_169_170_171_172_173_174_175_176_177_178_179_180_181_182_183_184_185_186_187_188_189_190_191_192_193_194_195_196_197_198_199_200_201_202_203_204_205_206_207_208_209_210_211_212_213_214_215_216_217_218_219_220_221_222_223_224_225_226_227_228_229_230_231_232_233_234_235_236_237_238_239_240_241_242_243_244_245_246_247_248_249_250_251_252_253_254_255_256_257_258_259_260_261_262_263_264_265_266_267_268_269_270_271_272_273_274_275_276_277_278_279_280_281_282_283_284_285_286_287_288_289_290_291_292_293_294_295_296_297_298_299_300_301_302_303_304_305_306_307_308_309_310_311_312_313_314_315_316_317_318_319_320_321_322_323_324_325_326_327_328_329_330_331_332_333_334_335_336_337_338_339_340_341_342_343_344_345_346_347_348_349_350_351_352_353_354_355_356_357_358_359_360_361_362_363_364_365_366_367_368_369_370_371_372_373_374_375_376_377_378_379_380_381_382_383_384_385_386_387_388_389_390_391_392_393_394_395_396_397_398_399_400_401_402_403_404_405_406_407_408_409_410_411_412_413_414_415_416_417_418_419_420_421_422_423_424_425_426_427_428_429_430_431_432_433_434_435_436_437_438_439_440_441_442_443_444_445_446_447_448_449_450_451_452_453_454_455_456_457_458_459_460_461_462_463_464_465_466_467_468_469_470_471_472_473_474_475_476_477_478_479_480_481_482_483_484_485_486_487_488_489_490_491_492_493_494_495_496_497_498_499_500_501_502_503_504_505_506_507_508_509_510_511_512_513_514_515_516_517_518_519_520_521_522_523_524_525_526_527_528_529_530_531_532_533_534_535_536_537_538_539_540_541_542_543_544_545_546_547_548_549_550_551_552_553_554_555_556_557_558_559_560_561_562_563_564_565_566_567_568_569_570_571_572_573_574_575_576_577_578_579_580_581_582_583_584_585_586_587_588_589_590_591_592_593_594_595_596_597_598_599_600_601_602_603_604_605_606_607_608_609_610_611_612_613_614_615_616_617_618_619_620_621_622_623_624_625_626_627_628_629_630_631_632_633_634_635_636_637_638_639_640_641_642_643_644_645_646_647_648_649_650_651_652_653_654_655_656_657_658_659_660_661_662_663_664_665_666_667_668_669_670_671_672_673_674_675_676_677_678_679_680_681_682_683_684_685_686_687_688_689_690_691_692_693_694_695_696_697_698_699_700_701_702_703_704_705_706_707_708_709_710_711_712_713_714_715_716_717_718_719_720_721_722_723_724_725_726_727_728_729_730_731_732_733_734_735_736_737_738_739_740_741_742_743_744_745_746_747_748_749_750_751_752_753_754_755_756_757_758_759_760_761_762_763_764_765_766_767_768_769_770_771_772_773_774_775_776_777_778_779_780_781_782_783_784_785_786_787_788_789_790_791_792_793_794_795_796_797_798_799_800_801_802_803_804_805_806_807_808_809_810_811_812_813_814_815_816_817_818_819_820_821_822_823_824_825_826_827_828_829_830_831_832_833_834_835_836_837_838_839_840_841_842_843_844_845_846_847_848_849_850_851_852_853_854_855_856_857_858_859_860_861_862_863_864_865_866_867_868_869_870_871_872_873_874_875_876_877_878_879_880_881_882_883_884_885_886_887_888_889_890_891_892_893_894_895_896_897_898_899_900_901_902_903_904_905_906_907_908_909_910_911_912_913_914_915_916_917_918_919_920_921_922_923_924_925_926_927_928_929_930_931_932_933_934_935_936_937_938_939_940_941_942_943_944_945_946_947_948_949_950_951_952_953_954_955_956_957_958_959_960_961_962_963_964_965_966_967_968_969_970_971_972_973_974_975_976_977_978_979_980_981_982_983_984_985_986_987_988_989_990_991_992_993_994_995_996_997_998_999_"
     ]
    }
   ],
   "source": [
    "dict_of_metrics_out_years20 = out_years_estimation(1000, 20)\n",
    "\n",
    "file_name = \"dict_of_metrics_out_years20.pkl\"\n",
    "output = open(file_name, 'wb') #\n",
    "pickle.dump(dict_of_metrics_out_years20, output)\n",
    "output.close()\n",
    "\n",
    "del(dict_of_metrics_out_years20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observation/conflicts discarded: 6887\n",
      "Number of observation preserved: 1260853\n",
      "Number of conflicts preserved: 6410\n",
      "0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43_44_45_46_47_48_49_50_51_52_53_54_55_56_57_58_59_60_61_62_63_64_65_66_67_68_69_70_71_72_73_74_75_76_77_78_79_80_81_82_83_84_85_86_87_88_89_90_91_92_93_94_95_96_97_98_99_100_101_102_103_104_105_106_107_108_109_110_111_112_113_114_115_116_117_118_119_120_121_122_123_124_125_126_127_128_129_130_131_132_133_134_135_136_137_138_139_140_141_142_143_144_145_146_147_148_149_150_151_152_153_154_155_156_157_158_159_160_161_162_163_164_165_166_167_168_169_170_171_172_173_174_175_176_177_178_179_180_181_182_183_184_185_186_187_188_189_190_191_192_193_194_195_196_197_198_199_200_201_202_203_204_205_206_207_208_209_210_211_212_213_214_215_216_217_218_219_220_221_222_223_224_225_226_227_228_229_230_231_232_233_234_235_236_237_238_239_240_241_242_243_244_245_246_247_248_249_250_251_252_253_254_255_256_257_258_259_260_261_262_263_264_265_266_267_268_269_270_271_272_273_274_275_276_277_278_279_280_281_282_283_284_285_286_287_288_289_290_291_292_293_294_295_296_297_298_299_300_301_302_303_304_305_306_307_308_309_310_311_312_313_314_315_316_317_318_319_320_321_322_323_324_325_326_327_328_329_330_331_332_333_334_335_336_337_338_339_340_341_342_343_344_345_346_347_348_349_350_351_352_353_354_355_356_357_358_359_360_361_362_363_364_365_366_367_368_369_370_371_372_373_374_375_376_377_378_379_380_381_382_383_384_385_386_387_388_389_390_391_392_393_394_395_396_397_398_399_400_401_402_403_404_405_406_407_408_409_410_411_412_413_414_415_416_417_418_419_420_421_422_423_424_425_426_427_428_429_430_431_432_433_434_435_436_437_438_439_440_441_442_443_444_445_446_447_448_449_450_451_452_453_454_455_456_457_458_459_460_461_462_463_464_465_466_467_468_469_470_471_472_473_474_475_476_477_478_479_480_481_482_483_484_485_486_487_488_489_490_491_492_493_494_495_496_497_498_499_500_501_502_503_504_505_506_507_508_509_510_511_512_513_514_515_516_517_518_519_520_521_522_523_524_525_526_527_528_529_530_531_532_533_534_535_536_537_538_539_540_541_542_543_544_545_546_547_548_549_550_551_552_553_554_555_556_557_558_559_560_561_562_563_564_565_566_567_568_569_570_571_572_573_574_575_576_577_578_579_580_581_582_583_584_585_586_587_588_589_590_591_592_593_594_595_596_597_598_599_600_601_602_603_604_605_606_607_608_609_610_611_612_613_614_615_616_617_618_619_620_621_622_623_624_625_626_627_628_629_630_631_632_633_634_635_636_637_638_639_640_641_642_643_644_645_646_647_648_649_650_651_652_653_654_655_656_657_658_659_660_661_662_663_664_665_666_667_668_669_670_671_672_673_674_675_676_677_678_679_680_681_682_683_684_685_686_687_688_689_690_691_692_693_694_695_696_697_698_699_700_701_702_703_704_705_706_707_708_709_710_711_712_713_714_715_716_717_718_719_720_721_722_723_724_725_726_727_728_729_730_731_732_733_734_735_736_737_738_739_740_741_742_743_744_745_746_747_748_749_750_751_752_753_754_755_756_757_758_759_760_761_762_763_764_765_766_767_768_769_770_771_772_773_774_775_776_777_778_779_780_781_782_783_784_785_786_787_788_789_790_791_792_793_794_795_796_797_798_799_800_801_802_803_804_805_806_807_808_809_810_811_812_813_814_815_816_817_818_819_820_821_822_823_824_825_826_827_828_829_830_831_832_833_834_835_836_837_838_839_840_841_842_843_844_845_846_847_848_849_850_851_852_853_854_855_856_857_858_859_860_861_862_863_864_865_866_867_868_869_870_871_872_873_874_875_876_877_878_879_880_881_882_883_884_885_886_887_888_889_890_891_892_893_894_895_896_897_898_899_900_901_902_903_904_905_906_907_908_909_910_911_912_913_914_915_916_917_918_919_920_921_922_923_924_925_926_927_928_929_930_931_932_933_934_935_936_937_938_939_940_941_942_943_944_945_946_947_948_949_950_951_952_953_954_955_956_957_958_959_960_961_962_963_964_965_966_967_968_969_970_971_972_973_974_975_976_977_978_979_980_981_982_983_984_985_986_987_988_989_990_991_992_993_994_995_996_997_998_999_"
     ]
    }
   ],
   "source": [
    "dict_of_metrics_out_years_onsets5 = out_years_estimation_onsets(1000,5)\n",
    "\n",
    "file_name = \"dict_of_metrics_out_years_onsets5.pkl\"\n",
    "output = open(file_name, 'wb') #\n",
    "pickle.dump(dict_of_metrics_out_years_onsets5, output)\n",
    "output.close()\n",
    "\n",
    "del(dict_of_metrics_out_years_onsets5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
